{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qp0H_zUQuu_"
   },
   "source": [
    "# Нейронные сети\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[HSE][ML][MS][HW05] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "Для начала вам предстоит реализовать свой собственный backpropagation и протестировать его на реальных данных, а затем научиться обучать нейронные сети при помощи библиотеки `PyTorch` и использовать это умение для классификации классического набора данных CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "22ezVRf3QuvA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from typing import List, NoReturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qfDPH_LQuvF"
   },
   "source": [
    "### Задание 1 (3 балла)\n",
    "Нейронные сети состоят из слоев, поэтому для начала понадобится реализовать их. Пока нам понадобятся только три:\n",
    "\n",
    "`Linear` - полносвязный слой, в котором `y = Wx + b`, где `y` - выход, `x` - вход, `W` - матрица весов, а `b` - смещение. \n",
    "\n",
    "`ReLU` - слой, соответствующий функции активации `y = max(0, x)`.\n",
    "\n",
    "`Softmax` - слой, соответствующий функции активации [softmax](https://ru.wikipedia.org/wiki/Softmax)\n",
    "\n",
    "\n",
    "#### Методы\n",
    "`forward(X)` - возвращает предсказанные для `X`. `X` может быть как вектором, так и батчем\n",
    "\n",
    "`backward(d)` - считает градиент при помощи обратного распространения ошибки. Возвращает новое значение `d`\n",
    "\n",
    "`update(alpha)` - обновляет веса (если необходимо) с заданой скоростью обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "id": "RWFLlHqaYbgC"
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \"\"\"\n",
    "    Абстрактный класс. Его менять не нужно.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, d):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def update(self, alpha):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "id": "aYS2gE4PYepZ"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Линейный полносвязный слой.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            Размер входа.\n",
    "        out_features : int \n",
    "            Размер выхода.\n",
    "    \n",
    "        Notes\n",
    "        -----\n",
    "        W и b инициализируются случайно.\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(in_features, out_features)\n",
    "        self.b = np.random.randn(out_features)[np.newaxis, :] \n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Возвращает y = Wx + b.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Входной вектор или батч.\n",
    "            То есть, либо x вектор с in_features элементов,\n",
    "            либо матрица размерности (batch_size, in_features).\n",
    "    \n",
    "        Return\n",
    "        ------\n",
    "        y : np.ndarray\n",
    "            Выход после слоя.\n",
    "            Либо вектор с out_features элементами,\n",
    "            либо матрица размерности (batch_size, out_features)\n",
    "\n",
    "        \"\"\"\n",
    "        if debug_on:\n",
    "            print(\"LINEAR_FORWARD. IN: \",x.shape)\n",
    "            print(\"LINEAR_FORWARD. OUT: \",(np.matmul(x, self.W) + self.b).shape)\n",
    "        self.x = x\n",
    "        \n",
    "        if self.x.ndim == 1:\n",
    "            self.x = self.x[np.newaxis, :]\n",
    "        \n",
    "        return np.matmul(x, self.W) + self.b\n",
    "    \n",
    "    def backward(self, d: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Cчитает градиент при помощи обратного распространения ошибки.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d : np.ndarray\n",
    "            Градиент.\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Новое значение градиента.\n",
    "        \"\"\"\n",
    "        \n",
    "        if d.ndim == 1:\n",
    "            d = d[np.newaxis,:]\n",
    "            \n",
    "\n",
    "        \n",
    "        if debug_on:\n",
    "            print(\"LINEAR_BACKWARD. IN: d=\",d.shape)\n",
    "#             print(\"LINEAR_BACKWARD. self.x.T=\",self.x.T)\n",
    "#             print(\"LINEAR_BACKWARD. self.W=\",self.W)\n",
    "#             print('x.T shape', self.x.T.shape)\n",
    "#             print('d shape:', d.shape)\n",
    "            print(\"LINEAR_BACKWARD. OUT:\",np.matmul(d, self.W.T).shape)\n",
    "            \n",
    "        self.grad_W = np.matmul(self.x.T, d)\n",
    "        self.grad_b = d\n",
    "        return np.matmul(d, self.W.T)\n",
    "        \n",
    "    def update(self, alpha: float) -> NoReturn:\n",
    "        \"\"\"\n",
    "        Обновляет W и b с заданной скоростью обучения.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float\n",
    "            Скорость обучения.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.W -= alpha * self.grad_W\n",
    "        self.b -= alpha * self.grad_b.sum(axis=0)[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "id": "94hkbnD1QuvG"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    Слой, соответствующий функции активации ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Возвращает y = max(0, x).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Входной вектор или батч.\n",
    "    \n",
    "        Return\n",
    "        ------\n",
    "        y : np.ndarray\n",
    "            Выход после слоя (той же размерности, что и вход).\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        y = np.copy(x)\n",
    "        y[y < 0] = 0\n",
    "        \n",
    "        self.y = y\n",
    "        \n",
    "        if debug_on == True:\n",
    "            print('RELU_FORWARD, IN: ', x.shape)\n",
    "            print('RELU_FORWARD, OUT: ', y.shape)\n",
    "            \n",
    "        return y\n",
    "        \n",
    "    def backward(self, d) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Cчитает градиент при помощи обратного распространения ошибки.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d : np.ndarray\n",
    "            Градиент.\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Новое значение градиента.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.y[self.y > 0] = 1\n",
    "        \n",
    "        grad = self.y\n",
    "        \n",
    "        if debug_on == True:\n",
    "            print('RELU_BACKWARD, IN: ', d.shape)\n",
    "#             print('RELU_BACKWARD, grad=', grad)\n",
    "            print('RELU_BACKWARD, OUT: ', (d * grad).shape)\n",
    "        \n",
    "        return d * grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "id": "94hkbnD1QuvG"
   },
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \"\"\"\n",
    "    Слой, соответствующий функции активации Softmax.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Возвращает y = Softmax(x).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Входной вектор или батч.\n",
    "    \n",
    "        Return\n",
    "        ------\n",
    "        y : np.ndarray\n",
    "            Выход после слоя (той же размерности, что и вход).\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        if debug_on:\n",
    "            print('SOFTMAX_FORWARD, IN: ', x.shape)\n",
    "            print('SOFTMAX_FORWARD, OUT: ', (np.exp(x)/np.exp(x).sum()).shape )\n",
    "        return np.exp(x)/np.exp(x).sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    def backward(self, d) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Cчитает градиент при помощи обратного распространения ошибки.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d : np.ndarray\n",
    "            Градиент.\n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Новое значение градиента.\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(self.x)\n",
    "        grad = exp_x * exp_x.sum(axis=1)[:, np.newaxis] - exp_x * exp_x\n",
    "        grad /= exp_x.sum(axis=1)[:, np.newaxis]**2\n",
    "        \n",
    "        if debug_on == True:\n",
    "            print('SOFTMAX_BACKWARD, IN: ', d.shape)\n",
    "#             print('SOFTMAX_BACKWARD, grad=', grad.shape)\n",
    "            \n",
    "            print('SOFTMAX_BACKWARD, OUT: ', (d * grad).shape)\n",
    "        return d * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb_ip_h8QuvJ"
   },
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь сделаем саму нейронную сеть.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - обучает нейронную сеть заданное число эпох. В каждой эпохе необходимо использовать [cross-entropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy) для обучения, а так же производить обновления не по одному элементу, а используя батчи.\n",
    "\n",
    "`predict_proba(X)` - предсказывает вероятности классов для элементов `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`modules` - список, состоящий из ранее реализованных модулей и описывающий слои нейронной сети. В конец необходимо добавить `Softmax`\n",
    "\n",
    "`epochs` - количество эпох обучения\n",
    "\n",
    "`alpha` - скорость обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches():\n",
    "    batch_start = 0\n",
    "    batch_end = batch_size\n",
    "\n",
    "    while True:\n",
    "        if batch_start >= 100:\n",
    "            return\n",
    "        if batch_end > 100:\n",
    "            yield batch_start, 100\n",
    "            return \n",
    "        yield batch_start, batch_end\n",
    "        batch_start += batch_size\n",
    "        batch_end += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "id": "Q_JFCizKQuvK"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "    def __init__(self, modules: List[Module], epochs: int = 40, alpha: float = 0.01):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        modules : List[Module]\n",
    "            Cписок, состоящий из ранее реализованных модулей и \n",
    "            описывающий слои нейронной сети. \n",
    "            В конец необходимо добавить Softmax.\n",
    "        epochs : int\n",
    "            Количество эпох обучения\n",
    "        alpha : float\n",
    "            Cкорость обучения.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.modules = modules + [Softmax()]\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "            \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, batch_size=32) -> NoReturn:\n",
    "        \"\"\"\n",
    "        Обучает нейронную сеть заданное число эпох. \n",
    "        В каждой эпохе необходимо использовать cross-entropy loss для обучения, \n",
    "        а так же производить обновления не по одному элементу, а используя батчи.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Данные для обучения.\n",
    "        y : np.ndarray\n",
    "            Вектор меток классов для данных.\n",
    "        batch_size : int\n",
    "            Размер батча.\n",
    "        \"\"\"\n",
    "        \n",
    "        X = np.copy(X)\n",
    "        \n",
    "        def get_batches():\n",
    "            batch_start = 0\n",
    "            batch_end = batch_size\n",
    "            \n",
    "            while True:\n",
    "                if batch_start >= len(X):\n",
    "                    return\n",
    "                if batch_end > len(X):\n",
    "                    yield batch_start, len(X)\n",
    "                    return \n",
    "                yield batch_start, batch_end\n",
    "                batch_start += batch_size\n",
    "                batch_end += batch_size\n",
    "        \n",
    "        y_vect = np.zeros((len(y), len(np.unique(y))))\n",
    "        for i in range(len(y)):\n",
    "            y_vect[i, y[i]] = 1\n",
    "                          \n",
    "        for i in range(self.epochs):\n",
    "#             np.random.shuffle(X)\n",
    "            for start, end in get_batches():\n",
    "                result = X[start:end]\n",
    "                for module in self.modules:\n",
    "                    result = module.forward(result)\n",
    "                          \n",
    "                y_true = np.copy(y_vect[start:end])\n",
    "                \n",
    "#                 LOSS\n",
    "#                 print(-1*np.log(result[:,y_true]))\n",
    "                if loss:\n",
    "                    print('LOSS:',-1*(y_true * np.log(result)).mean())\n",
    "                     \n",
    "                          \n",
    "#                 производная кросс-энтропии\n",
    "                y_true[y_true != 0] /= result[y_true != 0]\n",
    "                d = y_true * -1             \n",
    "                          \n",
    "#                 d = np.zeros(result.shape)\n",
    "#                 d[0,y_true] = -1/result[0,y_true]\n",
    "                \n",
    "                if debug_on:\n",
    "                    print('-----------BACKWARD-----------BACKWARD-----------BACKWARD-----------')\n",
    "                \n",
    "                for module in self.modules[::-1]:\n",
    "                    d = module.backward(d)\n",
    "                    module.update(self.alpha)\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Предсказывает вероятности классов для элементов X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Данные для предсказания.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Предсказанные вероятности классов для всех элементов X.\n",
    "            Размерность (X.shape[0], n_classes)\n",
    "        \n",
    "        \"\"\"\n",
    "        all_ = []\n",
    "        result = X\n",
    "        for module in self.modules:\n",
    "            result = module.forward(result)\n",
    "        \n",
    "        return result\n",
    "            \n",
    "        \n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Предсказывает метки классов для элементов X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Данные для предсказания.\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        np.ndarray\n",
    "            Вектор предсказанных классов\n",
    "        \n",
    "        \"\"\"\n",
    "        p = self.predict_proba(X)\n",
    "        return np.argmax(p, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 13.306191323704688\n",
      "LOSS: 15.51391296094377\n",
      "LOSS: 13.243556618408078\n",
      "LOSS: 15.402366700464146\n",
      "LOSS: 13.181154308511713\n",
      "LOSS: 15.291026590530313\n",
      "LOSS: 13.118804475201994\n",
      "LOSS: 15.180456440159219\n",
      "LOSS: 13.056585856112905\n",
      "LOSS: 15.070032131314877\n",
      "LOSS: 12.994483722442265\n",
      "LOSS: 14.959719781936615\n",
      "LOSS: 12.932485071938011\n",
      "LOSS: 14.849491271284482\n",
      "LOSS: 12.870573610138045\n",
      "LOSS: 14.73932567232378\n",
      "LOSS: 12.80873658382159\n",
      "LOSS: 14.629263297200374\n",
      "LOSS: 12.746964957332043\n",
      "LOSS: 14.519315812370419\n",
      "LOSS: 12.685252526277306\n",
      "LOSS: 14.408874322338656\n",
      "LOSS: 12.623701986232266\n",
      "LOSS: 14.296871561821316\n",
      "LOSS: 12.562206087824022\n",
      "LOSS: 14.185102897986571\n",
      "LOSS: 12.500765882239278\n",
      "LOSS: 14.073655661081734\n",
      "LOSS: 12.439385162722347\n",
      "LOSS: 13.962654459927872\n",
      "LOSS: 12.37807082964081\n",
      "LOSS: 13.852153448197079\n",
      "LOSS: 12.316156939453016\n",
      "LOSS: 13.738947389313463\n",
      "LOSS: 12.25433271434583\n",
      "LOSS: 13.626835817197863\n",
      "LOSS: 12.192613787532384\n",
      "LOSS: 13.516096440786036\n",
      "LOSS: 12.131011638452577\n",
      "LOSS: 13.40693848425031\n",
      "LOSS: 12.069474741921049\n",
      "LOSS: 13.29956676897924\n",
      "LOSS: 12.008076829812321\n",
      "LOSS: 13.194018360347968\n",
      "LOSS: 11.946815671911956\n",
      "LOSS: 13.09019788967386\n",
      "LOSS: 11.885681833619593\n",
      "LOSS: 12.987908365993102\n",
      "LOSS: 11.824661255098661\n",
      "LOSS: 12.886900667421829\n",
      "LOSS: 11.763738256693522\n",
      "LOSS: 12.786920621665328\n",
      "LOSS: 11.702898209645957\n",
      "LOSS: 12.687736723558364\n",
      "LOSS: 11.642128039004952\n",
      "LOSS: 12.58915678082127\n",
      "LOSS: 11.581417467700092\n",
      "LOSS: 12.491029165862116\n",
      "LOSS: 11.520740926925036\n",
      "LOSS: 12.393210836287606\n",
      "LOSS: 11.460096546286518\n",
      "LOSS: 12.295645186788342\n",
      "LOSS: 11.39949471922046\n",
      "LOSS: 12.198271017693543\n",
      "LOSS: 11.338915753715998\n",
      "LOSS: 12.101032480743282\n",
      "LOSS: 11.278362032058679\n",
      "LOSS: 12.003912611415746\n",
      "LOSS: 11.21784832163685\n",
      "LOSS: 11.906891759163543\n",
      "LOSS: 11.157376143566413\n",
      "LOSS: 11.809957727377187\n",
      "LOSS: 11.096948174997701\n",
      "LOSS: 11.713103949121454\n",
      "LOSS: 11.036564747530337\n",
      "LOSS: 11.616283768858786\n",
      "LOSS: 10.976092997114712\n",
      "LOSS: 11.51954326587056\n",
      "LOSS: 10.915682254922164\n",
      "LOSS: 11.422886761162834\n",
      "LOSS: 10.855341732489059\n",
      "LOSS: 11.326321396061307\n",
      "LOSS: 10.795083363218826\n",
      "LOSS: 11.229857411665291\n",
      "LOSS: 10.734923036731685\n",
      "LOSS: 11.133509529385442\n",
      "LOSS: 10.674883741356577\n",
      "LOSS: 11.03730153522509\n",
      "LOSS: 10.615004826932948\n",
      "LOSS: 10.941280755661438\n",
      "LOSS: 10.555370722237267\n",
      "LOSS: 10.845562922814842\n",
      "LOSS: 10.496198301040481\n",
      "LOSS: 10.750464123090794\n",
      "LOSS: 10.438197891602261\n",
      "LOSS: 10.656736615884268\n",
      "LOSS: 10.382882587088496\n",
      "LOSS: 10.566217859791657\n",
      "LOSS: 10.331940141310646\n",
      "LOSS: 10.480734450115737\n",
      "LOSS: 10.285930985251923\n",
      "LOSS: 10.399984372092053\n",
      "LOSS: 10.24313986707334\n",
      "LOSS: 10.322309004836727\n",
      "LOSS: 10.201930023116567\n",
      "LOSS: 10.246794586701803\n",
      "LOSS: 10.161700537982206\n",
      "LOSS: 10.172921961476435\n",
      "LOSS: 10.122023451331295\n",
      "LOSS: 10.100600489549215\n",
      "LOSS: 10.082711383895141\n",
      "LOSS: 10.029960525933639\n",
      "LOSS: 10.043665753925268\n",
      "LOSS: 9.961195902420018\n",
      "LOSS: 10.00482794616625\n",
      "LOSS: 9.894441622100734\n",
      "LOSS: 9.966159385021705\n",
      "LOSS: 9.829699904997533\n",
      "LOSS: 9.927633113887135\n",
      "LOSS: 9.766831336440557\n",
      "LOSS: 9.889229674530378\n",
      "LOSS: 9.705598713831538\n",
      "LOSS: 9.850934521697111\n",
      "LOSS: 9.645727469036464\n",
      "LOSS: 9.812831979878307\n",
      "LOSS: 9.587076618436704\n",
      "LOSS: 9.77485056623375\n",
      "LOSS: 9.529290285173508\n",
      "LOSS: 9.736949433803627\n",
      "LOSS: 9.472180208427192\n",
      "LOSS: 9.699120781158872\n",
      "LOSS: 9.415597591648144\n",
      "LOSS: 9.661357458741167\n",
      "LOSS: 9.35942711225554\n",
      "LOSS: 9.623652884521931\n",
      "LOSS: 9.303580315355841\n",
      "LOSS: 9.586001002512086\n",
      "LOSS: 9.24798969333174\n",
      "LOSS: 9.548396256710003\n",
      "LOSS: 9.192603825370373\n",
      "LOSS: 9.510833568273384\n",
      "LOSS: 9.137383553778527\n",
      "LOSS: 9.473329611277181\n",
      "LOSS: 9.082326715317793\n",
      "LOSS: 9.435915294538368\n",
      "LOSS: 9.027382880810007\n",
      "LOSS: 9.398530308392084\n",
      "LOSS: 8.972534656460647\n",
      "LOSS: 9.361171223347132\n",
      "LOSS: 8.917768732333261\n",
      "LOSS: 9.32383495033975\n",
      "LOSS: 8.863074905448485\n",
      "LOSS: 9.286518712321952\n",
      "LOSS: 8.808445339576272\n",
      "LOSS: 9.249220016962026\n",
      "LOSS: 8.753874004409191\n",
      "LOSS: 9.211936630760977\n",
      "LOSS: 8.699356251158093\n",
      "LOSS: 9.174666554739654\n",
      "LOSS: 8.644888492460458\n",
      "LOSS: 9.137408004187996\n",
      "LOSS: 8.59046763359033\n",
      "LOSS: 9.10015904152545\n",
      "LOSS: 8.536091914289166\n",
      "LOSS: 9.062918588014\n",
      "LOSS: 8.481759727464542\n",
      "LOSS: 9.025685373054813\n",
      "LOSS: 8.427469897637554\n",
      "LOSS: 8.988458260248002\n",
      "LOSS: 8.373221617695712\n",
      "LOSS: 8.951236231731114\n",
      "LOSS: 8.319014452480403\n",
      "LOSS: 8.914018367618056\n",
      "LOSS: 8.264848358867695\n",
      "LOSS: 8.87680383383436\n",
      "LOSS: 8.21072367772391\n",
      "LOSS: 8.839591862287623\n",
      "LOSS: 8.156641240242074\n",
      "LOSS: 8.802381731296322\n",
      "LOSS: 8.102602487050044\n",
      "LOSS: 8.765172743133363\n",
      "LOSS: 8.048609638914721\n",
      "LOSS: 8.727964197506344\n",
      "LOSS: 7.994665931773814\n",
      "LOSS: 8.690755359281466\n",
      "LOSS: 7.940775932432451\n",
      "LOSS: 8.653545418463018\n",
      "LOSS: 7.887056498411549\n",
      "LOSS: 8.616418900539\n",
      "LOSS: 7.833682642718085\n",
      "LOSS: 8.579288961496633\n",
      "LOSS: 7.780387959265724\n",
      "LOSS: 8.542154214449491\n",
      "LOSS: 7.727187842188313\n",
      "LOSS: 8.50501291179641\n",
      "LOSS: 7.674102519331026\n",
      "LOSS: 8.467862831903085\n",
      "LOSS: 7.621158205927135\n",
      "LOSS: 8.43070115122996\n",
      "LOSS: 7.568388312405105\n",
      "LOSS: 8.393524311387178\n",
      "LOSS: 7.515834533005182\n",
      "LOSS: 8.356327900393449\n",
      "LOSS: 7.463547520185106\n",
      "LOSS: 8.319106580523435\n",
      "LOSS: 7.411586715548564\n",
      "LOSS: 8.281854109064659\n",
      "LOSS: 7.360018818294293\n",
      "LOSS: 8.244563506427442\n",
      "LOSS: 7.3090899004867635\n",
      "LOSS: 8.207269878858868\n",
      "LOSS: 7.258851926183884\n",
      "LOSS: 8.169923413837642\n",
      "LOSS: 7.209204611568745\n",
      "LOSS: 8.132517892507412\n",
      "LOSS: 7.160193235961013\n",
      "LOSS: 8.0950487625702\n",
      "LOSS: 7.11184225811238\n",
      "LOSS: 8.057513755218743\n",
      "LOSS: 7.06415279977182\n",
      "LOSS: 8.01991322283411\n",
      "LOSS: 7.017103822055425\n",
      "LOSS: 7.982250135469658\n",
      "LOSS: 6.970656425946446\n",
      "LOSS: 7.944529778822112\n",
      "LOSS: 6.924759837082011\n",
      "LOSS: 7.906759269604748\n",
      "LOSS: 6.879357488762638\n",
      "LOSS: 7.868947019921762\n",
      "LOSS: 6.834392070607455\n",
      "LOSS: 7.83110225146952\n",
      "LOSS: 6.789809064070788\n",
      "LOSS: 7.79323461215402\n",
      "LOSS: 6.745558807358058\n",
      "LOSS: 7.755353905874869\n",
      "LOSS: 6.701597402812191\n",
      "LOSS: 7.7174699204893145\n",
      "LOSS: 6.657886842158914\n",
      "LOSS: 7.679592328246098\n",
      "LOSS: 6.614394673114572\n",
      "LOSS: 7.641730632017905\n",
      "LOSS: 6.57109344170583\n",
      "LOSS: 7.603894134462621\n",
      "LOSS: 6.52796006039745\n",
      "LOSS: 7.566091912575226\n",
      "LOSS: 6.484996683215361\n",
      "LOSS: 7.52834029065066\n",
      "LOSS: 6.442445918276273\n",
      "LOSS: 7.490659794211004\n",
      "LOSS: 6.400225640640957\n",
      "LOSS: 7.4530395853512665\n",
      "LOSS: 6.358738999143446\n",
      "LOSS: 7.415549196546541\n",
      "LOSS: 6.317424760369176\n",
      "LOSS: 7.378134493118816\n",
      "LOSS: 6.276121251413645\n",
      "LOSS: 7.340801702533266\n",
      "LOSS: 6.234901558444752\n",
      "LOSS: 7.303556637851142\n",
      "LOSS: 6.193923620304367\n",
      "LOSS: 7.266437567749199\n",
      "LOSS: 6.153058839356842\n",
      "LOSS: 7.229415122988071\n",
      "LOSS: 6.112265190723461\n",
      "LOSS: 7.192492219743658\n",
      "LOSS: 6.0715402662449405\n",
      "LOSS: 7.155670713433255\n",
      "LOSS: 6.030876725405259\n",
      "LOSS: 7.118951592228326\n",
      "LOSS: 5.990280260283599\n",
      "LOSS: 7.082334139831813\n",
      "LOSS: 5.949750533145659\n",
      "LOSS: 7.045817253183266\n",
      "LOSS: 5.90928741924232\n",
      "LOSS: 7.0093989784165665\n",
      "LOSS: 5.868891311334066\n",
      "LOSS: 6.973076623984792\n",
      "LOSS: 5.828563089173547\n",
      "LOSS: 6.936846882910128\n",
      "LOSS: 5.788304098995058\n",
      "LOSS: 6.900705957085689\n",
      "LOSS: 5.748116141859049\n",
      "LOSS: 6.864649677755488\n",
      "LOSS: 5.708001469705639\n",
      "LOSS: 6.828673617865274\n",
      "LOSS: 5.667962787968894\n",
      "LOSS: 6.792773193634648\n",
      "LOSS: 5.628003263632992\n",
      "LOSS: 6.756943754234537\n",
      "LOSS: 5.588126537702861\n",
      "LOSS: 6.721180659725968\n",
      "LOSS: 5.548336741236522\n",
      "LOSS: 6.685479348362339\n",
      "LOSS: 5.508638514353501\n",
      "LOSS: 6.6498353949709355\n",
      "LOSS: 5.469037027987426\n",
      "LOSS: 6.614251799278621\n",
      "LOSS: 5.429531594672628\n",
      "LOSS: 6.578732747800689\n",
      "LOSS: 5.389862796000423\n",
      "LOSS: 6.543219496844817\n",
      "LOSS: 5.350361167593718\n",
      "LOSS: 6.507679574116235\n",
      "LOSS: 5.310981850922772\n",
      "LOSS: 6.472179988323019\n",
      "LOSS: 5.271733099529198\n",
      "LOSS: 6.436738372785306\n",
      "LOSS: 5.232689034941732\n",
      "LOSS: 6.401566045464815\n",
      "LOSS: 5.193794055703864\n",
      "LOSS: 6.366446456795973\n",
      "LOSS: 5.155114852738965\n",
      "LOSS: 6.331477313613394\n",
      "LOSS: 5.116606725425726\n",
      "LOSS: 6.296539060904827\n",
      "LOSS: 5.078282233131948\n",
      "LOSS: 6.261631671292308\n",
      "LOSS: 5.0401548961491365\n",
      "LOSS: 6.226755736108086\n",
      "LOSS: 5.002239140410384\n",
      "LOSS: 6.191912470754632\n",
      "LOSS: 4.9645501631346605\n",
      "LOSS: 6.15710370165188\n",
      "LOSS: 4.927103696387194\n",
      "LOSS: 6.122331830743825\n",
      "LOSS: 4.889915650885259\n",
      "LOSS: 6.087599774580751\n",
      "LOSS: 4.853001633892368\n",
      "LOSS: 6.052910876980745\n",
      "LOSS: 4.8163763528992325\n",
      "LOSS: 6.018268797252069\n",
      "LOSS: 4.780052939255695\n",
      "LOSS: 5.983677379726761\n",
      "LOSS: 4.744867084027649\n",
      "LOSS: 5.949597953842072\n",
      "LOSS: 4.71065393568001\n",
      "LOSS: 5.915576632380297\n",
      "LOSS: 4.67677028073849\n",
      "LOSS: 5.88161703032904\n",
      "LOSS: 4.6432169661010505\n",
      "LOSS: 5.847722469276632\n",
      "LOSS: 4.609990933181665\n",
      "LOSS: 5.813895947898191\n",
      "LOSS: 4.577085431235098\n",
      "LOSS: 5.780140176622812\n",
      "LOSS: 4.544490482839248\n",
      "LOSS: 5.746457686125888\n",
      "LOSS: 4.5121935488793605\n",
      "LOSS: 5.712851017887921\n",
      "LOSS: 4.480180325760901\n",
      "LOSS: 5.679217504011442\n",
      "LOSS: 4.448381229396957\n",
      "LOSS: 5.645342571924274\n",
      "LOSS: 4.416835719816331\n",
      "LOSS: 5.611557931308588\n",
      "LOSS: 4.3855294842665\n",
      "LOSS: 5.577870256049508\n",
      "LOSS: 4.354471082632509\n",
      "LOSS: 5.544329116944965\n",
      "LOSS: 4.3237245417919965\n",
      "LOSS: 5.510825114223175\n",
      "LOSS: 4.293164252184572\n",
      "LOSS: 5.4774595746738\n",
      "LOSS: 4.262809177191526\n",
      "LOSS: 5.444257246213778\n",
      "LOSS: 4.232662098937799\n",
      "LOSS: 5.411251667053474\n",
      "LOSS: 4.202738462392535\n",
      "LOSS: 5.378485016909177\n",
      "LOSS: 4.173041075425368\n",
      "LOSS: 5.346007507356226\n",
      "LOSS: 4.143599351511508\n",
      "LOSS: 5.313881454407343\n",
      "LOSS: 4.114442490658071\n",
      "LOSS: 5.282172879220379\n",
      "LOSS: 4.085603811021939\n",
      "LOSS: 5.250946444678574\n",
      "LOSS: 4.057117070685165\n",
      "LOSS: 5.220269783600474\n",
      "LOSS: 4.029008308185565\n",
      "LOSS: 5.190165976477538\n",
      "LOSS: 4.001308257897764\n",
      "LOSS: 5.1606464830610275\n",
      "LOSS: 3.974012636039263\n",
      "LOSS: 5.131699659155542\n",
      "LOSS: 3.947113134583024\n",
      "LOSS: 5.103267832021548\n",
      "LOSS: 3.920608807785271\n",
      "LOSS: 5.075336865230486\n",
      "LOSS: 3.8944792384522\n",
      "LOSS: 5.047859098551127\n",
      "LOSS: 3.868700342175594\n",
      "LOSS: 5.020788070818823\n",
      "LOSS: 3.843247244350034\n",
      "LOSS: 4.994082131007775\n",
      "LOSS: 3.818096290063042\n",
      "LOSS: 4.9677061001967875\n",
      "LOSS: 3.7932262162158867\n",
      "LOSS: 4.941631586999826\n",
      "LOSS: 3.7686186707118563\n",
      "LOSS: 4.91580334871612\n",
      "LOSS: 3.7442211772006058\n",
      "LOSS: 4.889960417858683\n",
      "LOSS: 3.7200641427153016\n",
      "LOSS: 4.864389531204953\n",
      "LOSS: 3.696138017146054\n",
      "LOSS: 4.839084125319495\n",
      "LOSS: 3.6724351758986606\n",
      "LOSS: 4.81404013612544\n",
      "LOSS: 3.648949470630908\n",
      "LOSS: 4.7892551001412\n",
      "LOSS: 3.625675804210018\n",
      "LOSS: 4.764727360253244\n",
      "LOSS: 3.602609750817781\n",
      "LOSS: 4.740455388426097\n",
      "LOSS: 3.5797472345974333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 4.716437234983177\n",
      "LOSS: 3.5570842733451946\n",
      "LOSS: 4.692670111243003\n",
      "LOSS: 3.534616788509723\n",
      "LOSS: 4.669150107236884\n",
      "LOSS: 3.512340478168094\n",
      "LOSS: 4.6458720394512785\n",
      "LOSS: 3.4902507459556364\n",
      "LOSS: 4.62282941619554\n",
      "LOSS: 3.4683426763538416\n",
      "LOSS: 4.600014501762136\n",
      "LOSS: 3.4466110454586043\n",
      "LOSS: 4.5774184562270275\n",
      "LOSS: 3.4250503563376378\n",
      "LOSS: 4.555031526148203\n",
      "LOSS: 3.4036548891226066\n",
      "LOSS: 4.5328432625107755\n",
      "LOSS: 3.382418757715711\n",
      "LOSS: 4.510842745461138\n",
      "LOSS: 3.3613359670364926\n",
      "LOSS: 4.4890187998242235\n",
      "LOSS: 3.3404004667558294\n",
      "LOSS: 4.467396055936628\n",
      "LOSS: 3.31980382065704\n",
      "LOSS: 4.44643570793291\n",
      "LOSS: 3.299337407368494\n",
      "LOSS: 4.4256156235529\n",
      "LOSS: 3.278995484017224\n",
      "LOSS: 4.404925629541774\n",
      "LOSS: 3.258772422836242\n",
      "LOSS: 4.384355983982328\n",
      "LOSS: 3.2386627295753687\n",
      "LOSS: 4.363897425014692\n",
      "LOSS: 3.2186610585271693\n",
      "LOSS: 4.343541201255725\n",
      "LOSS: 3.1987622252386125\n",
      "LOSS: 4.323279087365139\n",
      "LOSS: 3.1789612179098556\n",
      "LOSS: 4.303103388005563\n",
      "LOSS: 3.159253208411387\n",
      "LOSS: 4.283006933119952\n",
      "LOSS: 3.1396335638096446\n",
      "LOSS: 4.262983067081172\n",
      "LOSS: 3.12009785902471\n",
      "LOSS: 4.243025634526782\n",
      "LOSS: 3.1006418794816812\n",
      "LOSS: 4.223128962214611\n",
      "LOSS: 3.0812616835267095\n",
      "LOSS: 4.203287842452909\n",
      "LOSS: 3.0619535773500575\n",
      "LOSS: 4.18349751691612\n",
      "LOSS: 3.0427141634777906\n",
      "LOSS: 4.163805774088149\n",
      "LOSS: 3.0236544674039387\n",
      "LOSS: 4.144261337007814\n",
      "LOSS: 3.004657623355489\n",
      "LOSS: 4.124755489661202\n",
      "LOSS: 2.9857214069141467\n",
      "LOSS: 4.105285212687825\n",
      "LOSS: 2.9668441275649813\n",
      "LOSS: 4.085878814910271\n",
      "LOSS: 2.9480145602027625\n",
      "LOSS: 4.066502771715984\n",
      "LOSS: 2.929242434034359\n",
      "LOSS: 4.047156007500041\n",
      "LOSS: 2.910476056707591\n",
      "LOSS: 4.027836096461575\n",
      "LOSS: 2.8917491991323114\n",
      "LOSS: 4.008543801412294\n",
      "LOSS: 2.873086614537021\n",
      "LOSS: 3.989279310617473\n",
      "LOSS: 2.8544936472315214\n",
      "LOSS: 3.970043758976439\n",
      "LOSS: 2.8359779797393947\n",
      "LOSS: 3.9508392179329785\n",
      "LOSS: 2.817550198033843\n",
      "LOSS: 3.9316689677790677\n",
      "LOSS: 2.799224425614876\n",
      "LOSS: 3.9125377309014757\n",
      "LOSS: 2.781019013838323\n",
      "LOSS: 3.8934519290059595\n",
      "LOSS: 2.7629572273174627\n",
      "LOSS: 3.874419942260949\n",
      "LOSS: 2.7450678150478764\n",
      "LOSS: 3.855452330299193\n",
      "LOSS: 2.727385295857547\n",
      "LOSS: 3.836561953359514\n",
      "LOSS: 2.7099497264164345\n",
      "LOSS: 3.8177639095666507\n",
      "LOSS: 2.692805687944304\n",
      "LOSS: 3.799075193107211\n",
      "LOSS: 2.6759963894979184\n",
      "LOSS: 3.7804924959889723\n",
      "LOSS: 2.6594571111121548\n",
      "LOSS: 3.762056733733344\n",
      "LOSS: 2.6433217707685244\n",
      "LOSS: 3.7437822445521802\n",
      "LOSS: 2.6276535318349987\n",
      "LOSS: 3.725683462311866\n",
      "LOSS: 2.612474260176244\n",
      "LOSS: 3.7077706656445324\n",
      "LOSS: 2.5977917079408015\n",
      "LOSS: 3.6900490699058546\n",
      "LOSS: 2.5835994449480637\n",
      "LOSS: 3.6725185580128006\n",
      "LOSS: 2.569878695403462\n",
      "LOSS: 3.6551740643085675\n",
      "LOSS: 2.5566014733323814\n",
      "LOSS: 3.638006462923575\n",
      "LOSS: 2.5437341834132545\n",
      "LOSS: 3.6210037245100466\n",
      "LOSS: 2.5312409565594796\n",
      "LOSS: 3.6041521081561387\n",
      "LOSS: 2.5190862826657128\n",
      "LOSS: 3.5874372193306687\n",
      "LOSS: 2.507236809161819\n",
      "LOSS: 3.5708448464855542\n",
      "LOSS: 2.495662385082213\n",
      "LOSS: 3.554361557056371\n",
      "LOSS: 2.484336525954135\n",
      "LOSS: 3.537975076146097\n",
      "LOSS: 2.4732287075139543\n",
      "LOSS: 3.521667204910134\n",
      "LOSS: 2.4623261752341703\n",
      "LOSS: 3.505437941435698\n",
      "LOSS: 2.4516173541109203\n",
      "LOSS: 3.48927874010883\n",
      "LOSS: 2.4410885611591593\n",
      "LOSS: 3.473182488927531\n",
      "LOSS: 2.4307288369810633\n",
      "LOSS: 3.4571433914862286\n",
      "LOSS: 2.4205295622816596\n",
      "LOSS: 3.4411568303045237\n",
      "LOSS: 2.4104840969904386\n",
      "LOSS: 3.4252192222084554\n",
      "LOSS: 2.4005874472926116\n",
      "LOSS: 3.4093278709952024\n",
      "LOSS: 2.3908359485273065\n",
      "LOSS: 3.3934808501315334\n",
      "LOSS: 2.3812269861245765\n",
      "LOSS: 3.377676848533069\n",
      "LOSS: 2.3717587294834668\n",
      "LOSS: 3.3619150575244054\n",
      "LOSS: 2.36242989045252\n",
      "LOSS: 3.3461950588454297\n",
      "LOSS: 2.353239504479272\n",
      "LOSS: 3.330516722782889\n",
      "LOSS: 2.344186735714704\n",
      "LOSS: 3.3148801168146296\n",
      "LOSS: 2.335270707817834\n",
      "LOSS: 3.2992854251179238\n",
      "LOSS: 2.3264903622583937\n",
      "LOSS: 3.283732879252187\n",
      "LOSS: 2.3178443455482154\n",
      "LOSS: 3.2682227002348103\n",
      "LOSS: 2.309330926095644\n",
      "LOSS: 3.2527550520687694\n",
      "LOSS: 2.300947940376572\n",
      "LOSS: 3.2373300065563266\n",
      "LOSS: 2.292692766990287\n",
      "LOSS: 3.2219475189644893\n",
      "LOSS: 2.284562326069217\n",
      "LOSS: 3.2066074138257297\n",
      "LOSS: 2.276553100576837\n",
      "LOSS: 3.1913093798947036\n",
      "LOSS: 2.2686611753608545\n",
      "LOSS: 3.176052973067617\n",
      "LOSS: 2.2608822894861014\n",
      "LOSS: 3.1608376259261775\n",
      "LOSS: 2.2532118973599427\n",
      "LOSS: 3.145662662503385\n",
      "LOSS: 2.245645234445602\n",
      "LOSS: 3.1305273168815306\n",
      "LOSS: 2.238177383868948\n",
      "LOSS: 3.11543075431512\n",
      "LOSS: 2.230803340879641\n",
      "LOSS: 3.100372093706022\n",
      "LOSS: 2.223518072846353\n",
      "LOSS: 3.0852937124374273\n",
      "LOSS: 2.2162816527948426\n",
      "LOSS: 3.070128256874714\n",
      "LOSS: 2.2091248527202176\n",
      "LOSS: 3.0550022692456897\n",
      "LOSS: 2.2020541457489506\n",
      "LOSS: 3.0399192407249314\n",
      "LOSS: 2.1950420896307956\n",
      "LOSS: 3.024858393922464\n",
      "LOSS: 2.188095461812539\n",
      "LOSS: 3.0098348346539017\n",
      "LOSS: 2.181221242356747\n",
      "LOSS: 2.994851562301738\n",
      "LOSS: 2.1743925428604665\n",
      "LOSS: 2.9798887911237433\n",
      "LOSS: 2.1676167504689157\n",
      "LOSS: 2.964961940523466\n",
      "LOSS: 2.160901477697847\n",
      "LOSS: 2.950073543519688\n",
      "LOSS: 2.1542204732240933\n",
      "LOSS: 2.9352049790915684\n",
      "LOSS: 2.147581771647148\n",
      "LOSS: 2.920371860184133\n",
      "LOSS: 2.1409936242772276\n",
      "LOSS: 2.905576741579016\n",
      "LOSS: 2.1344303978780483\n",
      "LOSS: 2.8908017573046783\n",
      "LOSS: 2.127900732671724\n",
      "LOSS: 2.8760625098634636\n",
      "LOSS: 2.121413461801022\n",
      "LOSS: 2.8613625968675094\n",
      "LOSS: 2.114943501216318\n",
      "LOSS: 2.8466838514214485\n",
      "LOSS: 2.108500019752742\n",
      "LOSS: 2.8320415786777913\n",
      "LOSS: 2.1020923506773226\n",
      "LOSS: 2.817442046975805\n",
      "LOSS: 2.0956958738040994\n",
      "LOSS: 2.802865016911025\n",
      "LOSS: 2.0893202012304384\n",
      "LOSS: 2.7883251308437673\n",
      "LOSS: 2.082975081889398\n",
      "LOSS: 2.7738336331929934\n",
      "LOSS: 2.07663627684028\n",
      "LOSS: 2.7593656644379876\n",
      "LOSS: 2.0703137615668035\n",
      "LOSS: 2.74493583887164\n",
      "LOSS: 2.0640062475707257\n",
      "LOSS: 2.7305524835227475\n",
      "LOSS: 2.057723920880173\n",
      "LOSS: 2.7162191068276265\n",
      "LOSS: 2.0514454816480834\n",
      "LOSS: 2.7019265144607534\n",
      "LOSS: 2.045178726930502\n",
      "LOSS: 2.687678011760832\n",
      "LOSS: 2.0389340981820956\n",
      "LOSS: 2.673480495984471\n",
      "LOSS: 2.032687975874305\n",
      "LOSS: 2.6593130316753166\n",
      "LOSS: 2.0264509350226367\n",
      "LOSS: 2.6451881910761816\n",
      "LOSS: 2.0202222390846214\n",
      "LOSS: 2.6311101317733\n",
      "LOSS: 2.0140125937205986\n",
      "LOSS: 2.6170819823800566\n",
      "LOSS: 2.0077986094063482\n",
      "LOSS: 2.603085713486357\n",
      "LOSS: 2.0015910900191436\n",
      "LOSS: 2.5891321229176816\n",
      "LOSS: 1.9953895061758509\n",
      "LOSS: 2.5752224127692704\n",
      "LOSS: 1.9892047616385413\n",
      "LOSS: 2.561366050127267\n",
      "LOSS: 1.9830136342207827\n",
      "LOSS: 2.5475377336867413\n",
      "LOSS: 1.9768270990838464\n",
      "LOSS: 2.5337498865798667\n",
      "LOSS: 1.9706447803873748\n",
      "LOSS: 2.5200016481935714\n",
      "LOSS: 1.9644663338315462\n",
      "LOSS: 2.5062927632083443\n",
      "LOSS: 1.9583028419586361\n",
      "LOSS: 2.492590874618804\n",
      "LOSS: 1.9521170857265402\n",
      "LOSS: 2.4787953068604938\n",
      "LOSS: 1.945934370911074\n",
      "LOSS: 2.465035530516176\n",
      "LOSS: 1.9397544532584492\n",
      "LOSS: 2.4513103032596346\n",
      "LOSS: 1.9335771099285897\n",
      "LOSS: 2.437618347890331\n",
      "LOSS: 1.9274021377354755\n",
      "LOSS: 2.423959811174327\n",
      "LOSS: 1.9212407585142028\n",
      "LOSS: 2.4103437551848055\n",
      "LOSS: 1.9150699943738454\n",
      "LOSS: 2.3967436864080147\n",
      "LOSS: 1.9089010938313118\n",
      "LOSS: 2.3831716666332987\n",
      "LOSS: 1.9027339170508257\n",
      "LOSS: 2.3696264073211113\n",
      "LOSS: 1.896568336753275\n",
      "LOSS: 2.3561066423849635\n",
      "LOSS: 1.8904042370988645\n",
      "LOSS: 2.3426111337822344\n",
      "LOSS: 1.88424151265821\n",
      "LOSS: 2.329138676064639\n",
      "LOSS: 1.8780800674665008\n",
      "LOSS: 2.3156895535475277\n",
      "LOSS: 1.8719312339281349\n",
      "LOSS: 2.302272645550928\n",
      "LOSS: 1.8657720973083702\n",
      "LOSS: 2.2888624092615077\n",
      "LOSS: 1.8596140004225599\n",
      "LOSS: 2.2754707916428147\n",
      "LOSS: 1.8534568771926736\n",
      "LOSS: 2.262096788103001\n",
      "LOSS: 1.8473006673466932\n",
      "LOSS: 2.248739437136388\n",
      "LOSS: 1.8411453158672895\n",
      "LOSS: 2.2353978201257463\n",
      "LOSS: 1.8349907724903427\n",
      "LOSS: 2.2220710608048364\n",
      "LOSS: 1.828836991249375\n",
      "LOSS: 2.208758324446942\n",
      "LOSS: 1.8226839300616808\n",
      "LOSS: 2.195458816837766\n",
      "LOSS: 1.8165315503534576\n",
      "LOSS: 2.1821717830830547\n",
      "LOSS: 1.8103798167197733\n",
      "LOSS: 2.1688965062953267\n",
      "LOSS: 1.804228696616794\n",
      "LOSS: 2.155632306196942\n",
      "LOSS: 1.798078160083237\n",
      "LOSS: 2.142378537671187\n",
      "LOSS: 1.7919281794883473\n",
      "LOSS: 2.1291345924218925\n",
      "LOSS: 1.7857787307860455\n",
      "LOSS: 2.1159001852869657\n",
      "LOSS: 1.779641232074069\n",
      "LOSS: 2.1026878322617\n",
      "LOSS: 1.7734927780669794\n",
      "LOSS: 2.08946995812242\n",
      "LOSS: 1.7673447882557727\n",
      "LOSS: 2.0762597681837747\n",
      "LOSS: 1.761197243672113\n",
      "LOSS: 2.0630567976735996\n",
      "LOSS: 1.7550501266025738\n",
      "LOSS: 2.0498606071183607\n",
      "LOSS: 1.7489034204653962\n",
      "LOSS: 2.0366707811215345\n",
      "LOSS: 1.7427571096993215\n",
      "LOSS: 2.023486927209307\n",
      "LOSS: 1.7366111796632286\n",
      "LOSS: 2.0103086747468093\n",
      "LOSS: 1.7304656165456995\n",
      "LOSS: 1.9971356739273678\n",
      "LOSS: 1.7243204072833125\n",
      "LOSS: 1.9839675948364666\n",
      "LOSS: 1.7181755394870288\n",
      "LOSS: 1.9708041265919116\n",
      "LOSS: 1.7120310013756532\n",
      "LOSS: 1.957644976561311\n",
      "LOSS: 1.7058867817158152\n",
      "LOSS: 1.944489869658002\n",
      "LOSS: 1.6997428697677512\n",
      "LOSS: 1.9313385477164269\n",
      "LOSS: 1.6935992552362835\n",
      "LOSS: 1.9181907689483257\n",
      "LOSS: 1.68745592822655\n",
      "LOSS: 1.9050463074812312\n",
      "LOSS: 1.6813128792038619\n",
      "LOSS: 1.8918946543831245\n",
      "LOSS: 1.675144348014824\n",
      "LOSS: 1.8785891745704149\n",
      "LOSS: 1.6689761229213464\n",
      "LOSS: 1.865286732058511\n",
      "LOSS: 1.6628081950622846\n",
      "LOSS: 1.8519871489045268\n",
      "LOSS: 1.656640555806862\n",
      "LOSS: 1.838690263167995\n",
      "LOSS: 1.650473196729125\n",
      "LOSS: 1.8253959289804305\n",
      "LOSS: 1.6443061095872928\n",
      "LOSS: 1.8121040167332931\n",
      "LOSS: 1.6381392862955666\n",
      "LOSS: 1.7988144133518638\n",
      "LOSS: 1.6319727189109463\n",
      "LOSS: 1.7855270227716742\n",
      "LOSS: 1.6258063996135657\n",
      "LOSS: 1.7722417664995127\n",
      "LOSS: 1.6196403206902792\n",
      "LOSS: 1.7589585843406046\n",
      "LOSS: 1.6134744745193987\n",
      "LOSS: 1.7456774352882984\n",
      "LOSS: 1.607308853556579\n",
      "LOSS: 1.7323982985896738\n",
      "LOSS: 1.6011434503216255\n",
      "LOSS: 1.7192582522603836\n",
      "LOSS: 1.595078485626619\n",
      "LOSS: 1.7062728811397068\n",
      "LOSS: 1.5890136335179526\n",
      "LOSS: 1.693289149698792\n",
      "LOSS: 1.5829488866915793\n",
      "LOSS: 1.6803071410623236\n",
      "LOSS: 1.576884237846317\n",
      "LOSS: 1.6673269651505165\n",
      "LOSS: 1.570819679674831\n",
      "LOSS: 1.6543487610429812\n",
      "LOSS: 1.564755204854906\n",
      "LOSS: 1.6413726996307754\n",
      "LOSS: 1.5586908060411082\n",
      "LOSS: 1.62839898658256\n",
      "LOSS: 1.5526264758520376\n",
      "LOSS: 1.615427865654115\n",
      "LOSS: 1.5465622068777176\n",
      "LOSS: 1.6024596223021126\n",
      "LOSS: 1.540497991656407\n",
      "LOSS: 1.589494587785518\n",
      "LOSS: 1.5344338226736502\n",
      "LOSS: 1.5765331436007806\n",
      "LOSS: 1.528369692355965\n",
      "LOSS: 1.563575726368917\n",
      "LOSS: 1.5223055930658327\n",
      "LOSS: 1.5506228331722038\n",
      "LOSS: 1.5162415170974342\n",
      "LOSS: 1.5376750273549944\n",
      "LOSS: 1.5101774566737596\n",
      "LOSS: 1.5247329447956453\n",
      "LOSS: 1.5041134039455144\n",
      "LOSS: 1.5117973006475742\n",
      "LOSS: 1.498049350992223\n",
      "LOSS: 1.4988688965352592\n",
      "LOSS: 1.4919852898261419\n",
      "LOSS: 1.485954522701894\n",
      "LOSS: 1.485922144493851\n",
      "LOSS: 1.4730414177672742\n",
      "LOSS: 1.4798589606311823\n",
      "LOSS: 1.4601385518775614\n",
      "LOSS: 1.4737957301640865\n",
      "LOSS: 1.4472471540673175\n",
      "LOSS: 1.467732445004961\n",
      "LOSS: 1.4343685811812077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 1.4616690971095043\n",
      "LOSS: 1.421504326335619\n",
      "LOSS: 1.455605678496075\n",
      "LOSS: 1.4086560270403576\n",
      "LOSS: 1.449542181309254\n",
      "LOSS: 1.39582547270663\n",
      "LOSS: 1.443478597890016\n",
      "LOSS: 1.3830146111978086\n",
      "LOSS: 1.4374149208630476\n",
      "LOSS: 1.370225554036797\n",
      "LOSS: 1.431351143244108\n",
      "LOSS: 1.357460579824985\n",
      "LOSS: 1.4252930543038267\n",
      "LOSS: 1.3447195560903409\n",
      "LOSS: 1.4191932728179764\n",
      "LOSS: 1.332007675146242\n",
      "LOSS: 1.4130934505350892\n",
      "LOSS: 1.3193277122528198\n",
      "LOSS: 1.406993605562295\n",
      "LOSS: 1.3066826030491832\n",
      "LOSS: 1.4008937396951302\n",
      "LOSS: 1.2940754050666037\n",
      "LOSS: 1.394793821418015\n",
      "LOSS: 1.2815093060871003\n",
      "LOSS: 1.38869385193959\n",
      "LOSS: 1.2689875912174546\n",
      "LOSS: 1.3825938347742384\n",
      "LOSS: 1.25651361409414\n",
      "LOSS: 1.3764937761381901\n",
      "LOSS: 1.2440907628658162\n",
      "LOSS: 1.3703936853758307\n",
      "LOSS: 1.231722421321471\n",
      "LOSS: 1.3642935754099021\n",
      "LOSS: 1.2194119258375613\n",
      "LOSS: 1.3581934632073371\n",
      "LOSS: 1.2071625191351796\n",
      "LOSS: 1.3520933702502211\n",
      "LOSS: 1.194977302143118\n",
      "LOSS: 1.3459933229995509\n",
      "LOSS: 1.1828591855241437\n",
      "LOSS: 1.3398933533382333\n",
      "LOSS: 1.1708108426094885\n",
      "LOSS: 1.333793498978856\n",
      "LOSS: 1.1588346655724222\n",
      "LOSS: 1.3276938038222692\n",
      "LOSS: 1.1469327266362128\n",
      "LOSS: 1.321594318253777\n",
      "LOSS: 1.1351067459463684\n",
      "LOSS: 1.3154950993660948\n",
      "LOSS: 1.1233580674482422\n",
      "LOSS: 1.3093962111008344\n",
      "LOSS: 1.1116856998156635\n",
      "LOSS: 1.3029722715567218\n",
      "LOSS: 1.099617550556281\n",
      "LOSS: 1.2965488316965144\n",
      "LOSS: 1.0876416596556917\n",
      "LOSS: 1.2901259821295654\n",
      "LOSS: 1.0757578433064687\n",
      "LOSS: 1.2837038214104295\n",
      "LOSS: 1.0639654173935138\n",
      "LOSS: 1.2772824556934033\n",
      "LOSS: 1.0522632365778266\n",
      "LOSS: 1.2708619983039222\n",
      "LOSS: 1.0406497424031786\n",
      "LOSS: 1.2644425692485162\n",
      "LOSS: 1.029126464005051\n",
      "LOSS: 1.2580359107296533\n",
      "LOSS: 1.0176971682355798\n",
      "LOSS: 1.2516189266364337\n",
      "LOSS: 1.0063370681697625\n",
      "LOSS: 1.2452033657009405\n",
      "LOSS: 0.9950564242614321\n",
      "LOSS: 1.2387893693063932\n",
      "LOSS: 0.9838524694850453\n",
      "LOSS: 1.232377082898731\n",
      "LOSS: 0.9727237487885745\n",
      "LOSS: 1.2259782781092676\n",
      "LOSS: 0.9616795223083506\n",
      "LOSS: 1.2195698669480248\n",
      "LOSS: 0.9506885101140865\n",
      "LOSS: 1.213163622514971\n",
      "LOSS: 0.9397628179505813\n",
      "LOSS: 1.2067597027166517\n",
      "LOSS: 0.9288997190727417\n",
      "LOSS: 1.2003582679129972\n",
      "LOSS: 0.9180965908952861\n",
      "LOSS: 1.1939594807679836\n",
      "LOSS: 0.9073548968461125\n",
      "LOSS: 1.1875751370732142\n",
      "LOSS: 0.8966766962428234\n",
      "LOSS: 1.1811821478485063\n",
      "LOSS: 0.8860390736621468\n",
      "LOSS: 1.1747923075311943\n",
      "LOSS: 0.875452305847783\n",
      "LOSS: 1.168405787463558\n",
      "LOSS: 0.864914502779085\n",
      "LOSS: 1.1618691324444406\n",
      "LOSS: 0.8544714994259638\n",
      "LOSS: 1.15450750751592\n",
      "LOSS: 0.8440852986069461\n",
      "LOSS: 1.1471387490530693\n",
      "LOSS: 0.8337262170830598\n",
      "LOSS: 1.1397752045597718\n",
      "LOSS: 0.8234081476425035\n",
      "LOSS: 1.1324171507077592\n",
      "LOSS: 0.8131301849512207\n",
      "LOSS: 1.1250648673723744\n",
      "LOSS: 0.802892251489651\n",
      "LOSS: 1.1177307284532492\n",
      "LOSS: 0.7927081191688341\n",
      "LOSS: 1.1103908487513627\n",
      "LOSS: 0.7825464089149867\n",
      "LOSS: 1.1030576001358068\n",
      "LOSS: 0.7724224622079746\n",
      "LOSS: 1.0957312768115681\n",
      "LOSS: 0.7623359886823842\n",
      "LOSS: 1.0884121771391537\n",
      "LOSS: 0.7522869748515928\n",
      "LOSS: 1.0811127035707655\n",
      "LOSS: 0.7422915582915206\n",
      "LOSS: 1.0738089764446301\n",
      "LOSS: 0.7323170013592996\n",
      "LOSS: 1.0665133982305823\n",
      "LOSS: 0.7223799370456596\n",
      "LOSS: 1.059226289894191\n",
      "LOSS: 0.7124807134002851\n",
      "LOSS: 1.0519479810858419\n",
      "LOSS: 0.7026198660611058\n",
      "LOSS: 1.0446788126004816\n",
      "LOSS: 0.6928039621451731\n",
      "LOSS: 1.0374312410077229\n",
      "LOSS: 0.6830336455564798\n",
      "LOSS: 1.030181451723394\n",
      "LOSS: 0.6732940006824877\n",
      "LOSS: 1.0229419339791468\n",
      "LOSS: 0.6635981328080249\n",
      "LOSS: 1.0157131248294526\n",
      "LOSS: 0.6543119184298964\n",
      "LOSS: 1.00870441709391\n",
      "LOSS: 0.6457066534824898\n",
      "LOSS: 1.0016949146410445\n",
      "LOSS: 0.6371353895026621\n",
      "LOSS: 0.9946971246711366\n",
      "LOSS: 0.6286125464010212\n",
      "LOSS: 0.9877115410975639\n",
      "LOSS: 0.6201431088121495\n",
      "LOSS: 0.980738718778741\n",
      "LOSS: 0.6117334304429527\n",
      "LOSS: 0.9737792914733046\n",
      "LOSS: 0.6033915509561292\n",
      "LOSS: 0.9668339940043563\n",
      "LOSS: 0.5951275385636419\n",
      "LOSS: 0.9599036887763057\n",
      "LOSS: 0.5869538270892805\n",
      "LOSS: 0.9529893961724891\n",
      "LOSS: 0.578885492440971\n",
      "LOSS: 0.9460923273341303\n",
      "LOSS: 0.5709403836360594\n",
      "LOSS: 0.9392139163308142\n",
      "LOSS: 0.563138993635584\n",
      "LOSS: 0.9323558468888472\n",
      "LOSS: 0.5555039385325957\n",
      "LOSS: 0.9255200670620753\n",
      "LOSS: 0.5480589312940265\n",
      "LOSS: 0.9187087843296644\n",
      "LOSS: 0.5408272115013399\n",
      "LOSS: 0.9119244346941119\n",
      "LOSS: 0.5338295346642364\n",
      "LOSS: 0.9051696233091666\n",
      "LOSS: 0.5270820068802591\n",
      "LOSS: 0.8984470407796805\n",
      "LOSS: 0.5205941994944515\n",
      "LOSS: 0.8917593665844714\n",
      "LOSS: 0.5143679997512627\n",
      "LOSS: 0.8851091758258742\n",
      "LOSS: 0.5083974957247683\n",
      "LOSS: 0.878498864994638\n",
      "LOSS: 0.5026699042785538\n",
      "LOSS: 0.8719306065309841\n",
      "LOSS: 0.4971672619118752\n",
      "LOSS: 0.8654063334616239\n",
      "LOSS: 0.4918684410199356\n",
      "LOSS: 0.8589277481413665\n",
      "LOSS: 0.48675107242179483\n",
      "LOSS: 0.8524963455803212\n",
      "LOSS: 0.4817930959344438\n",
      "LOSS: 0.8461134420765206\n",
      "LOSS: 0.47697383193752113\n",
      "LOSS: 0.8397802023953328\n",
      "LOSS: 0.47227459896022195\n",
      "LOSS: 0.8334976618141741\n",
      "LOSS: 0.46767897189733376\n",
      "LOSS: 0.8272667418762483\n",
      "LOSS: 0.46317279269972833\n",
      "LOSS: 0.8210882603005681\n",
      "LOSS: 0.458744032169858\n",
      "LOSS: 0.8149629362713839\n",
      "LOSS: 0.45438257684197764\n",
      "LOSS: 0.8088913925417457\n",
      "LOSS: 0.45007999028572865\n",
      "LOSS: 0.8028741556861131\n",
      "LOSS: 0.44582927842479547\n",
      "LOSS: 0.7969116556040695\n",
      "LOSS: 0.44162467453995236\n",
      "LOSS: 0.7910042251145304\n",
      "LOSS: 0.4374614506867467\n",
      "LOSS: 0.7851521002379206\n",
      "LOSS: 0.4333357569778335\n",
      "LOSS: 0.7793554215598318\n",
      "LOSS: 0.4292444873411365\n",
      "LOSS: 0.7736142369068326\n",
      "LOSS: 0.42518516903959436\n",
      "LOSS: 0.7679285054376289\n",
      "LOSS: 0.4211558727894608\n",
      "LOSS: 0.7622981031553848\n",
      "LOSS: 0.41715514033429046\n",
      "LOSS: 0.7567228297720557\n",
      "LOSS: 0.41318177047997584\n",
      "LOSS: 0.7512024572739681\n",
      "LOSS: 0.40923520966314386\n",
      "LOSS: 0.7457366199462176\n",
      "LOSS: 0.40531513809281366\n",
      "LOSS: 0.7403249418392893\n",
      "LOSS: 0.4014215107015581\n",
      "LOSS: 0.734967008609626\n",
      "LOSS: 0.39755456288240065\n",
      "LOSS: 0.7296623785380771\n",
      "LOSS: 0.39371479252769215\n",
      "LOSS: 0.7244105936718118\n",
      "LOSS: 0.38990294513509716\n",
      "LOSS: 0.7192111908153288\n",
      "LOSS: 0.3861200006595762\n",
      "LOSS: 0.7140637120848646\n",
      "LOSS: 0.38236716085459876\n",
      "LOSS: 0.7089677147328418\n",
      "LOSS: 0.37864583589510104\n",
      "LOSS: 0.703922779944099\n",
      "LOSS: 0.3749576291253726\n",
      "LOSS: 0.6989285203070525\n",
      "LOSS: 0.37130431884501186\n",
      "LOSS: 0.6939845856722938\n",
      "LOSS: 0.36768783615457123\n",
      "LOSS: 0.6890906671327318\n",
      "LOSS: 0.3641102380480685\n",
      "LOSS: 0.684246498896552\n",
      "LOSS: 0.360573675178395\n",
      "LOSS: 0.6794518578809636\n",
      "LOSS: 0.3570803423832201\n",
      "LOSS: 0.6747070821327443\n",
      "LOSS: 0.3536315626228215\n",
      "LOSS: 0.6700790662898344\n",
      "LOSS: 0.35020464383965305\n",
      "LOSS: 0.665607336414582\n",
      "LOSS: 0.34682686094110976\n",
      "LOSS: 0.6611813459345999\n",
      "LOSS: 0.3435000215252811\n",
      "LOSS: 0.6568010012737575\n",
      "LOSS: 0.3402257563597872\n",
      "LOSS: 0.6524662027192948\n",
      "LOSS: 0.33700547056049507\n",
      "LOSS: 0.6481768331809407\n",
      "LOSS: 0.33384029969786516\n",
      "LOSS: 0.6439327472693304\n",
      "LOSS: 0.33073107306897837\n",
      "LOSS: 0.6397337613234559\n",
      "LOSS: 0.3276782860779753\n",
      "LOSS: 0.6355796449807742\n",
      "LOSS: 0.3246820831697097\n",
      "LOSS: 0.6314701147988088\n",
      "LOSS: 0.32174225210764074\n",
      "LOSS: 0.6274048303087576\n",
      "LOSS: 0.31885822965001454\n",
      "LOSS: 0.6233833927247422\n",
      "LOSS: 0.31602911794226785\n",
      "LOSS: 0.6194053463612404\n",
      "LOSS: 0.3132537102927472\n",
      "LOSS: 0.6154701826457242\n",
      "LOSS: 0.31053052450035734\n",
      "LOSS: 0.6115773464682882\n",
      "LOSS: 0.3078578415989563\n",
      "LOSS: 0.607726244498738\n",
      "LOSS: 0.3052337477857571\n",
      "LOSS: 0.6039162550291111\n",
      "LOSS: 0.3026561773928007\n",
      "LOSS: 0.6001467388692905\n",
      "LOSS: 0.3001229550041053\n",
      "LOSS: 0.5964170508292223\n",
      "LOSS: 0.2976318351663028\n",
      "LOSS: 0.5927265513574322\n",
      "LOSS: 0.2951805385354124\n",
      "LOSS: 0.5890746179621789\n",
      "LOSS: 0.2927667837008814\n",
      "LOSS: 0.5854606561095893\n",
      "LOSS: 0.2903883142939941\n",
      "LOSS: 0.5818841093641227\n",
      "LOSS: 0.28804292129827996\n",
      "LOSS: 0.5783444686040792\n",
      "LOSS: 0.28572846072283764\n",
      "LOSS: 0.5748412802043057\n",
      "LOSS: 0.28344286697449445\n",
      "LOSS: 0.5713741531265507\n",
      "LOSS: 0.28118416237632393\n",
      "LOSS: 0.5679427648952369\n",
      "LOSS: 0.27895046333818196\n",
      "LOSS: 0.564546866461849\n",
      "LOSS: 0.2767399837006765\n",
      "LOSS: 0.5611862859769242\n",
      "LOSS: 0.2745692404007612\n",
      "LOSS: 0.5578822643645335\n",
      "LOSS: 0.27248132092156147\n",
      "LOSS: 0.5546115460515962\n",
      "LOSS: 0.2703764898162402\n",
      "LOSS: 0.5513630514918605\n",
      "LOSS: 0.26828888779716875\n",
      "LOSS: 0.548149875121667\n",
      "LOSS: 0.266217194360506\n",
      "LOSS: 0.5449722459761998\n",
      "LOSS: 0.2641601729307957\n",
      "LOSS: 0.5418304709764983\n",
      "LOSS: 0.26211666604592104\n",
      "LOSS: 0.5387261410036781\n",
      "LOSS: 0.26012218651007774\n",
      "LOSS: 0.535683634663864\n",
      "LOSS: 0.25810278668948267\n",
      "LOSS: 0.5326518227487657\n",
      "LOSS: 0.25609383916911793\n",
      "LOSS: 0.5296577457498235\n",
      "LOSS: 0.25409445612326637\n",
      "LOSS: 0.5267020309767118\n",
      "LOSS: 0.252103805302453\n",
      "LOSS: 0.5237853494358499\n",
      "LOSS: 0.25012110604810966\n",
      "LOSS: 0.5209274906216792\n",
      "LOSS: 0.24818204331797783\n",
      "LOSS: 0.5180979969071018\n",
      "LOSS: 0.24621327803700316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 0.5153023208321453\n",
      "LOSS: 0.2442503887814984\n",
      "LOSS: 0.5125485624101862\n",
      "LOSS: 0.24229276893376067\n",
      "LOSS: 0.5098374607014352\n",
      "LOSS: 0.2403398483466032\n",
      "LOSS: 0.5071997191656004\n",
      "LOSS: 0.23842674294902644\n",
      "LOSS: 0.504570180324474\n",
      "LOSS: 0.2364817916937795\n",
      "LOSS: 0.5019906091410286\n",
      "LOSS: 0.23454002466344134\n",
      "LOSS: 0.49945628396018005\n",
      "LOSS: 0.23260100011308948\n",
      "LOSS: 0.49699263682613903\n",
      "LOSS: 0.23069894981479153\n",
      "LOSS: 0.49454766263049055\n",
      "LOSS: 0.22876432287315643\n",
      "LOSS: 0.4921511226301226\n",
      "LOSS: 0.22683127836889846\n",
      "LOSS: 0.4898013575314052\n",
      "LOSS: 0.22489948352216824\n",
      "LOSS: 0.48753071781938295\n",
      "LOSS: 0.2230020077550753\n",
      "LOSS: 0.48526275829777227\n",
      "LOSS: 0.2210719240196883\n",
      "LOSS: 0.48305299670638085\n",
      "LOSS: 0.2191422389432226\n",
      "LOSS: 0.4809071460762648\n",
      "LOSS: 0.2172449424094344\n",
      "LOSS: 0.47879199881264195\n",
      "LOSS: 0.21531547217550306\n",
      "LOSS: 0.47672054606993364\n",
      "LOSS: 0.21338575928805942\n",
      "LOSS: 0.474705303772458\n",
      "LOSS: 0.21148663234807863\n",
      "LOSS: 0.47273068977517446\n",
      "LOSS: 0.20955603994338448\n",
      "LOSS: 0.4707929699220031\n",
      "LOSS: 0.20762475365664837\n",
      "LOSS: 0.4689104554716796\n",
      "LOSS: 0.20572240075752485\n",
      "LOSS: 0.4670623579945203\n",
      "LOSS: 0.20378949970004928\n",
      "LOSS: 0.46525128448145736\n",
      "LOSS: 0.2018556212230649\n",
      "LOSS: 0.46350026124399274\n",
      "LOSS: 0.1999491894684335\n",
      "LOSS: 0.46176361765940377\n",
      "LOSS: 0.19801327174149264\n",
      "LOSS: 0.46007126600516657\n",
      "LOSS: 0.1961037887401398\n",
      "LOSS: 0.45842733431342153\n",
      "LOSS: 0.19416569830961522\n",
      "LOSS: 0.4568065157700238\n",
      "LOSS: 0.19222646219623193\n",
      "LOSS: 0.45523914699052714\n",
      "LOSS: 0.19031245881181533\n",
      "LOSS: 0.453680018579455\n",
      "LOSS: 0.18837102959795957\n",
      "LOSS: 0.45216914700419486\n",
      "LOSS: 0.18645401198183156\n",
      "LOSS: 0.45068162402251516\n",
      "LOSS: 0.18451048009444923\n",
      "LOSS: 0.4492260209645125\n",
      "LOSS: 0.18259061235995178\n",
      "LOSS: 0.44780195739788237\n",
      "LOSS: 0.1806451352349815\n",
      "LOSS: 0.4464000850847039\n",
      "LOSS: 0.17872264689592962\n",
      "LOSS: 0.4450319032267578\n",
      "LOSS: 0.17677543881879015\n",
      "LOSS: 0.44368201386096534\n",
      "LOSS: 0.17485061416060613\n",
      "LOSS: 0.44236273277059923\n",
      "LOSS: 0.17290193821642885\n",
      "LOSS: 0.44106295469569523\n",
      "LOSS: 0.17097510875517927\n",
      "LOSS: 0.4397862134949269\n",
      "LOSS: 0.16902527168678005\n",
      "LOSS: 0.43853461902990337\n",
      "LOSS: 0.16709680828438475\n",
      "LOSS: 0.4372946073342701\n",
      "LOSS: 0.16514615459619308\n",
      "LOSS: 0.43608923069457123\n",
      "LOSS: 0.16321646590814076\n",
      "LOSS: 0.4348807158712999\n",
      "LOSS: 0.16126537826686965\n",
      "LOSS: 0.43371962899345656\n",
      "LOSS: 0.15933490881995213\n",
      "LOSS: 0.4325424843499218\n",
      "LOSS: 0.1574041097995893\n",
      "LOSS: 0.43139468253419466\n",
      "LOSS: 0.15545319901111856\n",
      "LOSS: 0.4302749756785894\n",
      "LOSS: 0.15352235877243656\n",
      "LOSS: 0.42914716984639406\n",
      "LOSS: 0.15157214298979874\n",
      "LOSS: 0.4280677293926301\n",
      "LOSS: 0.1496418034846913\n",
      "LOSS: 0.42696844136908424\n",
      "LOSS: 0.1477118211746342\n",
      "LOSS: 0.4258835560902424\n",
      "LOSS: 0.14576346215513275\n",
      "LOSS: 0.4248429435265298\n",
      "LOSS: 0.14383352608772831\n",
      "LOSS: 0.42378325824675867\n",
      "LOSS: 0.14190442382711008\n",
      "LOSS: 0.4227356808859114\n",
      "LOSS: 0.13997633608859306\n",
      "LOSS: 0.4217070156874857\n",
      "LOSS: 0.13803175229706846\n",
      "LOSS: 0.42070314388607843\n",
      "LOSS: 0.13610647411720342\n",
      "LOSS: 0.4196877882212251\n",
      "LOSS: 0.13418280245506034\n",
      "LOSS: 0.4186828803371156\n",
      "LOSS: 0.1322438462755365\n",
      "LOSS: 0.41771350029579923\n",
      "LOSS: 0.13032426184632503\n",
      "LOSS: 0.41672591533005154\n",
      "LOSS: 0.12840697965090694\n",
      "LOSS: 0.4157468873028129\n",
      "LOSS: 0.12649226111148756\n",
      "LOSS: 0.4147759547963664\n",
      "LOSS: 0.12458038314196142\n",
      "LOSS: 0.41381712392935144\n",
      "LOSS: 0.12265551775657899\n",
      "LOSS: 0.4128810301216391\n",
      "LOSS: 0.12075041106687334\n",
      "LOSS: 0.4119311366404936\n",
      "LOSS: 0.1188490765552798\n",
      "LOSS: 0.4110580103284128\n",
      "LOSS: 0.11694575031970382\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaziz\\AppData\\Local\\Temp/ipykernel_14088/2851664155.py:45: RuntimeWarning: overflow encountered in square\n",
      "  grad /= exp_x.sum(axis=1)[:, np.newaxis]**2\n",
      "C:\\Users\\gaziz\\AppData\\Local\\Temp/ipykernel_14088/2851664155.py:44: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = exp_x * exp_x.sum(axis=1)[:, np.newaxis] - exp_x * exp_x\n",
      "C:\\Users\\gaziz\\AppData\\Local\\Temp/ipykernel_14088/2851664155.py:44: RuntimeWarning: invalid value encountered in subtract\n",
      "  grad = exp_x * exp_x.sum(axis=1)[:, np.newaxis] - exp_x * exp_x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS: nan\n",
      "LOSS:"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14088/2478625062.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14088/2038493970.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, batch_size)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#                 print(-1*np.log(result[:,y_true]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LOSS:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m                 \u001b[1;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    539\u001b[0m                 )\n\u001b[0;32m    540\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p = MLPClassifier([\n",
    "    Linear(4, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 2)\n",
    "], alpha = 0.000006, epochs = 3000)\n",
    "\n",
    "X = np.random.randn(50, 4)\n",
    "y = [(0 if x[0] > x[2]**2 or x[3]**3 > 0.5 else 1) for x in X]\n",
    "p.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.modules[4].W.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C1EIsDqQuvQ"
   },
   "source": [
    "### Задание 3 (2 балла)\n",
    "Протестируем наше решение на синтетических данных. Необходимо подобрать гиперпараметры, при которых качество полученных классификаторов будет достаточным.\n",
    "\n",
    "#### Оценка\n",
    "Accuracy на первом датасете больше 0.85 - +1 балл\n",
    "\n",
    "Accuracy на втором датасете больше 0.85 - +1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "id": "d5UAgXTcQuvQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.91\n"
     ]
    }
   ],
   "source": [
    "X, y = make_moons(400, noise=0.075)\n",
    "X_test, y_test = make_moons(400, noise=0.075)\n",
    "\n",
    "best_acc = 0\n",
    "for _ in range(1):\n",
    "    p = MLPClassifier([\n",
    "        Linear(2, 10),\n",
    "        ReLU(),\n",
    "        Linear(10, 10),\n",
    "        ReLU(),\n",
    "        Linear(10, 2)\n",
    "    ], alpha = 0.000001, epochs = 5000)\n",
    "\n",
    "\n",
    "    p.fit(X, y, batch_size=400)\n",
    "    best_acc = max(np.mean(p.predict(X_test) == y_test), best_acc)\n",
    "print(\"Accuracy\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "id": "MMDJM4qFQuvT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.905\n"
     ]
    }
   ],
   "source": [
    "X, y = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])\n",
    "X_test, y_test = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])\n",
    "best_acc = 0\n",
    "for _ in range(1):\n",
    "    p = MLPClassifier([\n",
    "        Linear(2, 10),\n",
    "        ReLU(),\n",
    "        Linear(10, 10),\n",
    "        ReLU(),\n",
    "        Linear(10, 3)\n",
    "    ], alpha = 0.000001, epochs = 2000)\n",
    "\n",
    "    p.fit(X, y, batch_size=50)\n",
    "    best_acc = max(np.mean(p.predict(X_test) == y_test), best_acc)\n",
    "print(\"Accuracy\", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPbVTFnMQuvW"
   },
   "source": [
    "## PyTorch\n",
    "\n",
    "Для выполнения следующего задания понадобится PyTorch. [Инструкция по установке](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "Если у вас нет GPU, то можно использовать [Google Colab](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tV0mJLu-QuvX"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "VUC_QqpAQuva"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "t = transforms.ToTensor()\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=True, transform=t)\n",
    "train_loader = DataLoader(cifar_train, batch_size=1024, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "cifar_test = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=False, transform=t)\n",
    "test_loader = DataLoader(cifar_test, batch_size=1024, shuffle=False, pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGmpjcFfQuvd"
   },
   "source": [
    "### Задание 4 (3 балла)\n",
    "А теперь поработам с настоящими нейронными сетями и настоящими данными. Необходимо реализовать сверточную нейронную сеть, которая будет классифицировать изображения из датасета CIFAR10. Имплементируйте класс `Model` и функцию `calculate_loss`. \n",
    "\n",
    "Обратите внимание, что `Model` должна считать в конце `softmax`, т.к. мы решаем задачу классификации. Соответствеено, функция `calculate_loss` считает cross-entropy.\n",
    "\n",
    "Для успешного выполнения задания необходимо, чтобы `accuracy`, `mean precision` и `mean recall` были больше 0.5\n",
    "\n",
    "__Можно пользоваться всем содержимым библиотеки PyTorch.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3)\n",
    "conv2 = nn.Conv2d(in_channels=6, out_channels=18, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 3, 32, 32])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 6, 30, 30])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 6, 15, 15])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool(conv1(x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 18, 13, 13])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2(pool(conv1(x))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 18, 6, 6])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pool(conv2(pool(conv1(x))))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "5sRmTKwKQuve"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=18, kernel_size=3)\n",
    "        \n",
    "        self.lin1 = nn.Linear(18*6*6, 60)\n",
    "        self.lin2 = nn.Linear(60, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(self.conv1(x))\n",
    "#         x = self.relu(x)\n",
    "        x = self.pool(self.conv2(x))\n",
    "#         x = self.relu(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "def calculate_loss(X: torch.Tensor, y: torch.Tensor, model: Model):\n",
    "    \"\"\"\n",
    "    Cчитает cross-entropy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.Tensor\n",
    "        Данные для обучения.\n",
    "    y : torch.Tensor\n",
    "        Метки классов.\n",
    "    model : Model\n",
    "        Модель, которую будем обучать.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred = model.forward(X)\n",
    "    loss = F.cross_entropy(y_pred, y)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAsLmkUqQuvh"
   },
   "source": [
    "Теперь обучим нашу модель. Для этого используем ранее созданные batch loader'ы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "k5G8iMCeQuvh"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(epochs):\n",
    "        #Train\n",
    "        loss_mean = 0\n",
    "        elements = 0\n",
    "        for X, y in iter(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = calculate_loss(X, y, model)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_mean += loss.item() * len(X)\n",
    "            elements += len(X)\n",
    "        train_losses.append(loss_mean / elements)\n",
    "        #Test\n",
    "        loss_mean = 0 \n",
    "        elements = 0\n",
    "        for X, y in iter(test_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = calculate_loss(X, y, model)\n",
    "            loss_mean += loss.item() * len(X)\n",
    "            elements += len(X)\n",
    "        test_losses.append(loss_mean / elements)\n",
    "        print(\"Epoch\", i, \"| Train loss\", train_losses[-1], \"| Test loss\", test_losses[-1])\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "vmD9eWJOQuvl",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9984/2195870878.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9984/1258766462.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mloss_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0melements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gaziz\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m255\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "train_l, test_l = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJNAuHjNQuvn"
   },
   "source": [
    "Построим график функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "F6OEGqriQuvo"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(train_l)), train_l, label=\"train\")\n",
    "plt.plot(range(len(test_l)), test_l, label=\"test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miUxg0bDQuvs"
   },
   "source": [
    "И, наконец, посчитаем метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "UXSOJFI8Quvt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy 0.549\n",
      "Precision [0.44100119 0.73178458 0.49017467 0.40071878 0.50842946 0.47179946\n",
      " 0.7260274  0.63380282        nan 0.64154412]\n",
      "Recall [0.74  0.693 0.449 0.446 0.573 0.527 0.689 0.675 0.    0.698]\n",
      "Mean Precision nan\n",
      "Mean Recall 0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaziz\\AppData\\Local\\Temp/ipykernel_9984/1591066153.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  print(\"Precision\", true_positive / (true_positive + false_positive))\n",
      "C:\\Users\\gaziz\\AppData\\Local\\Temp/ipykernel_9984/1591066153.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
      "  print(\"Mean Precision\", np.mean(true_positive / (true_positive + false_positive)))\n"
     ]
    }
   ],
   "source": [
    "true_positive = np.zeros(10)\n",
    "true_negative = np.zeros(10)\n",
    "false_positive = np.zeros(10)\n",
    "false_negative = np.zeros(10)\n",
    "accuracy = 0\n",
    "ctn = 0\n",
    "for X, y in iter(test_loader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X).max(dim=1)[1]\n",
    "    for i in range(10):\n",
    "        for pred, real in zip(y_pred, y):\n",
    "            if real == i:\n",
    "                if pred == real:\n",
    "                    true_positive[i] += 1\n",
    "                else:\n",
    "                    false_negative[i] += 1\n",
    "            else:\n",
    "                if pred == i:\n",
    "                    false_positive[i] += 1\n",
    "                else:\n",
    "                    true_negative[i] += 1\n",
    "            \n",
    "    accuracy += torch.sum(y_pred == y).item()\n",
    "    ctn += len(y)\n",
    "print(\"Overall accuracy\", accuracy / ctn)\n",
    "print(\"Precision\", true_positive / (true_positive + false_positive))\n",
    "print(\"Recall\", true_positive / (true_positive + false_negative))\n",
    "print(\"Mean Precision\", np.mean(true_positive / (true_positive + false_positive)))\n",
    "print(\"Mean Recall\", np.mean(true_positive / (true_positive + false_negative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([938., 254., 467., 667., 554., 590., 260., 390.,   0., 390.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "EKA-j4rIQuvv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([740., 693., 449., 446., 573., 527., 689., 675.,   0., 698.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 260.,  307.,  551.,  554.,  427.,  473.,  311.,  325., 1000.,\n",
       "        302.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw05_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
